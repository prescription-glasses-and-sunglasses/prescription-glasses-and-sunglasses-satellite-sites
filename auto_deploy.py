import os
import json
import sys
import io
import random
import time
import re
import subprocess
import requests
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# ------------------------
# æœåŠ¡è´¦å·é…ç½®
# ------------------------
service_account_info = os.environ.get("GDRIVE_SERVICE_ACCOUNT")
if not service_account_info:
    print("âŒ æœªæ‰¾åˆ° GDRIVE_SERVICE_ACCOUNT ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

try:
    service_account_info = json.loads(service_account_info)
except json.JSONDecodeError:
    print("âŒ è§£æ GDRIVE_SERVICE_ACCOUNT å¤±è´¥ã€‚è¯·ç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²ã€‚")
    sys.exit(1)

SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
creds = service_account.Credentials.from_service_account_info(service_account_info, scopes=SCOPES)
service = build('drive', 'v3', credentials=creds)

# ------------------------
# æ”¯æŒå¤šæ–‡ä»¶å¤¹ ID
# ------------------------
folder_ids_str = os.environ.get("GDRIVE_FOLDER_ID")
if not folder_ids_str:
    print("âŒ æœªæ‰¾åˆ° GDRIVE_FOLDER_ID ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

FOLDER_IDS = [fid.strip() for fid in folder_ids_str.split(",") if fid.strip()]

# ------------------------
# ä» TXT æ–‡ä»¶è¯»å–å…³é”®è¯
# ------------------------
keywords = []
keywords_file = "keywords.txt"
if os.path.exists(keywords_file):
    with open(keywords_file, "r", encoding="utf-8") as f:
        keywords = [line.strip() for line in f if line.strip()]

if not keywords:
    print("âš ï¸ keywords.txt ä¸­æ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶åã€‚")

# ------------------------
# è®°å½•å·²å¤„ç†çš„æ–‡ä»¶ ID å’Œæ–‡ä»¶åˆ—è¡¨ç¼“å­˜
# ------------------------
processed_file_path = "processed_files.json"
cache_file_path = "files_cache.json"
CACHE_EXPIRY_HOURS = 24  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰

try:
    if os.path.exists(processed_file_path):
        with open(processed_file_path, "r") as f:
            processed_data = json.load(f)
    else:
        processed_data = {"fileIds": []}
except (json.JSONDecodeError, IOError) as e:
    print(f"è¯»å– {processed_file_path} æ—¶å‡ºé”™: {e}ã€‚å°†ä»ä¸€ä¸ªç©ºçš„å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨å¼€å§‹ã€‚")
    processed_data = {"fileIds": []}

def get_cached_files():
    """ä»ç¼“å­˜ä¸­è¯»å–æ–‡ä»¶åˆ—è¡¨ï¼Œå¦‚æœç¼“å­˜è¿‡æœŸåˆ™è¿”å›Noneã€‚"""
    if os.path.exists(cache_file_path):
        try:
            with open(cache_file_path, "r") as f:
                cache_data = json.load(f)
                last_updated = cache_data.get("last_updated")
                if last_updated and (time.time() - last_updated < CACHE_EXPIRY_HOURS * 3600):
                    print("âœ… ç¼“å­˜æœªè¿‡æœŸï¼Œæ­£åœ¨ä»æœ¬åœ°åŠ è½½æ–‡ä»¶åˆ—è¡¨ã€‚")
                    return cache_data.get("files", [])
                else:
                    print(f"â³ ç¼“å­˜å·²è¿‡æœŸï¼ˆä¸Šæ¬¡æ›´æ–°è¶…è¿‡ {CACHE_EXPIRY_HOURS} å°æ—¶ï¼‰ï¼Œå°†é‡æ–°æ‹‰å–æ–‡ä»¶åˆ—è¡¨ã€‚")
        except (json.JSONDecodeError, IOError) as e:
            print(f"è¯»å– {cache_file_path} æ—¶å‡ºé”™: {e}ã€‚å°†é‡æ–°æ‹‰å–æ–‡ä»¶åˆ—è¡¨ã€‚")
    return None

def save_files_to_cache(files):
    """å°†æ–‡ä»¶åˆ—è¡¨å’Œå½“å‰æ—¶é—´æˆ³ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ã€‚"""
    cache_data = {
        "last_updated": time.time(),
        "files": files
    }
    with open(cache_file_path, "w") as f:
        json.dump(cache_data, f, indent=4)
    print("ğŸ’¾ å·²å°†æ–‡ä»¶åˆ—è¡¨ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜ã€‚")

# ------------------------
# è·å–æ–‡ä»¶åˆ—è¡¨çš„å‡½æ•° (å·²ä¼˜åŒ–)
# ------------------------
def list_files(folder_id):
    """åˆ—å‡ºæŒ‡å®š Google Drive æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¯æŒåˆ†é¡µã€‚"""
    all_the_files = []
    page_token = None
    query = f"'{folder_id}' in parents and (" \
            "mimeType='text/html' or " \
            "mimeType='text/plain' or " \
            "mimeType='application/vnd.google-apps.document')"
    try:
        while True:
            results = service.files().list(
                q=query,
                pageSize=1000,
                fields="nextPageToken, files(id, name, mimeType)",
                pageToken=page_token
            ).execute()
            items = results.get('files', [])
            all_the_files.extend(items)
            page_token = results.get('nextPageToken', None)
            if page_token is None:
                break
        print(f"  - åœ¨æ–‡ä»¶å¤¹ {folder_id} ä¸­æ€»å…±æ‰¾åˆ° {len(all_the_files)} ä¸ªæ–‡ä»¶ã€‚")
        return all_the_files
    except Exception as e:
        print(f"åˆ—å‡ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

# ------------------------
# ä¸‹è½½å’Œç”Ÿæˆ HTML
# ------------------------
def download_html_file(file_id, file_name):
    """ä¸‹è½½ä¸€ä¸ª HTML æ–‡ä»¶ã€‚"""
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    print(f"âœ… å·²ä¸‹è½½ {file_name}")

def download_txt_file(file_id, file_name, original_name):
    """ä¸‹è½½ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶å¹¶å°†å…¶è½¬æ¢ä¸º HTMLã€‚"""
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    text_content = fh.getvalue().decode('utf-8')
    
    # æ£€æŸ¥å†…å®¹æ˜¯å¦å·²ç»æ˜¯HTMLæ ¼å¼
    is_html = text_content.strip().lower().startswith('<!doctype html') or text_content.strip().lower().startswith('<html')
    
    if is_html:
        # å¦‚æœå·²ç»æ˜¯HTMLæ ¼å¼ï¼Œç›´æ¥ä¿å­˜
        html_content = text_content
    else:
        # å¦‚æœä¸æ˜¯HTMLæ ¼å¼ï¼Œåˆ™åŒ…è£…æˆHTML
        html_content = f"<!DOCTYPE html><html><head><meta charset='utf-8'><title>{original_name}</title></head><body><pre>{text_content}</pre></body></html>"
    
    with open(file_name, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"âœ… TXT å·²è½¬æ¢ä¸º HTML: {file_name}")

def export_google_doc(file_id, file_name):
    """å°† Google æ–‡æ¡£å¯¼å‡ºä¸º HTMLã€‚"""
    request = service.files().export_media(fileId=file_id, mimeType='text/html')
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    print(f"âœ… Google æ–‡æ¡£å·²å¯¼å‡ºä¸º HTML: {file_name}")

# ------------------------
# éƒ¨ç½²åˆ°ç›®æ ‡å¹³å°
# ------------------------
def deploy_to_target(target):
    """ä½¿ç”¨ Vercel å’Œ Netlify CLI éƒ¨ç½²åˆ°æŒ‡å®šçš„é¡¹ç›®å’Œç«™ç‚¹ã€‚"""
    print(f"ğŸš€ æ­£åœ¨éƒ¨ç½²åˆ° Vercel é¡¹ç›®: {target['vercel_project_id']}")
    vercel_command = [
        "vercel", "--prod", "--yes",
        "--token", os.environ.get("VERCEL_TOKEN"),
        "--project", target["vercel_project_id"],
        "--scope", os.environ.get("VERCEL_ORG_ID")
    ]
    try:
        subprocess.run(vercel_command, check=True)
        print("âœ… Vercel éƒ¨ç½²æˆåŠŸï¼")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Vercel éƒ¨ç½²å¤±è´¥: {e}")
        return

    print(f"ğŸš€ æ­£åœ¨éƒ¨ç½²åˆ° Netlify ç«™ç‚¹: {target['netlify_site_id']}")
    netlify_command = [
        "netlify", "deploy", "--dir", ".", "--prod", "--site", target["netlify_site_id"]
    ]
    try:
        subprocess.run(netlify_command, check=True)
        print("âœ… Netlify éƒ¨ç½²æˆåŠŸï¼")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Netlify éƒ¨ç½²å¤±è´¥: {e}")

# ------------------------
# æ–°å¢çš„ API åˆ›å»ºå‡½æ•°
# ------------------------
def create_new_target_api(vercel_token, netlify_token, vercel_org_id):
    """
    é€šè¿‡ API åˆ›å»ºæ–°çš„ Vercel å’Œ Netlify é¡¹ç›®ã€‚
    """
    project_name = f"auto-site-{int(time.time())}"
    
    print("----------------------------------------------------------------------")
    print(f"ğŸš€ æ­£åœ¨é€šè¿‡ API åˆ›å»ºæ–°çš„ Vercel é¡¹ç›®: {project_name}")
    print("----------------------------------------------------------------------")

    # Vercel API è°ƒç”¨
    vercel_url = f"https://api.vercel.com/v9/projects?{f'teamId={vercel_org_id}' if vercel_org_id else ''}"
    vercel_headers = {
        "Authorization": f"Bearer {vercel_token}",
        "Content-Type": "application/json"
    }
    vercel_payload = {
        "name": project_name,
        "framework": None,
        "git": None
    }
    try:
        response = requests.post(vercel_url, headers=vercel_headers, json=vercel_payload)
        response.raise_for_status()
        vercel_data = response.json()
        new_vercel_project_id = vercel_data.get('id')
        print(f"âœ… Vercel é¡¹ç›®åˆ›å»ºæˆåŠŸï¼ŒID: {new_vercel_project_id}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Vercel API è°ƒç”¨å¤±è´¥: {e}")
        return None

    print("----------------------------------------------------------------------")
    print(f"ğŸš€ æ­£åœ¨é€šè¿‡ API åˆ›å»ºæ–°çš„ Netlify ç«™ç‚¹: {project_name}")
    print("----------------------------------------------------------------------")

    # Netlify API è°ƒç”¨
    netlify_url = "https://api.netlify.com/api/v1/sites"
    netlify_headers = {
        "Authorization": f"Bearer {netlify_token}",
        "Content-Type": "application/json"
    }
    netlify_payload = {
        "name": project_name
    }
    try:
        response = requests.post(netlify_url, headers=netlify_headers, json=netlify_payload)
        response.raise_for_status()
        netlify_data = response.json()
        new_netlify_site_id = netlify_data.get('site_id')
        print(f"âœ… Netlify ç«™ç‚¹åˆ›å»ºæˆåŠŸï¼ŒID: {new_netlify_site_id}")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Netlify API è°ƒç”¨å¤±è´¥: {e}")
        return None
    
    new_target = {
        "vercel_project_id": new_vercel_project_id,
        "netlify_site_id": new_netlify_site_id
    }
    
    deploy_targets_file = "deploy_targets.json"
    try:
        with open(deploy_targets_file, "r+") as f:
            targets = json.load(f)
            targets.append(new_target)
            f.seek(0)
            json.dump(targets, f, indent=4)
    except FileNotFoundError:
        with open(deploy_targets_file, "w") as f:
            json.dump([new_target], f, indent=4)
            
    print(f"\nâœ… å·²æˆåŠŸåˆ›å»ºå¹¶ä¿å­˜æ–°çš„éƒ¨ç½²ç›®æ ‡åˆ° {deploy_targets_file}ï¼")
    return new_target

# ------------------------
# ä¸»ç¨‹åº
# ------------------------
all_files = get_cached_files()

if all_files is None:
    all_files = []
    print("â³ æ­£åœ¨ä» Google Drive æ‹‰å–æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨...")
    for folder_id in FOLDER_IDS:
        print(f"ğŸ“‚ æ­£åœ¨è·å–æ–‡ä»¶å¤¹: {folder_id}")
        files = list_files(folder_id)
        all_files.extend(files)
    save_files_to_cache(all_files)

new_files = [f for f in all_files if f['id'] not in processed_data["fileIds"]]

if not new_files:
    print("âœ… æ²¡æœ‰æ–°çš„æ–‡ä»¶éœ€è¦å¤„ç†ã€‚")
else:
    print(f"å‘ç° {len(new_files)} ä¸ªæœªå¤„ç†æ–‡ä»¶ã€‚")
    num_to_process = min(len(new_files), 30)
    selected_files = random.sample(new_files, num_to_process)
    print(f"æœ¬æ¬¡è¿è¡Œå°†å¤„ç† {len(selected_files)} ä¸ªæ–‡ä»¶ã€‚")

    available_keywords = list(keywords)
    keywords_ran_out = False

    for f in selected_files:
        if available_keywords:
            keyword = available_keywords.pop(0)
            safe_name = keyword + ".html"
        else:
            if not keywords_ran_out:
                print("âš ï¸ å…³é”®è¯å·²ç”¨å®Œï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶ååŠ éšæœºåç¼€ã€‚")
                keywords_ran_out = True
            
            base_name = os.path.splitext(f['name'])[0]
            sanitized_name = base_name.replace(" ", "-").replace("/", "-")
            random_suffix = str(random.randint(1000, 9999))
            safe_name = f"{sanitized_name}-{random_suffix}.html"

        print(f"æ­£åœ¨å¤„ç† '{f['name']}' -> '{safe_name}'")

        if f['mimeType'] == 'text/html':
            download_html_file(f['id'], safe_name)
        elif f['mimeType'] == 'text/plain':
            download_txt_file(f['id'], safe_name, f['name'])
        else: # 'application/vnd.google-apps.document'
            export_google_doc(f['id'], safe_name)

        processed_data["fileIds"].append(f['id'])

    with open(processed_file_path, "w") as f:
        json.dump(processed_data, f, indent=4)
    print(f"ğŸ’¾ å·²å°† {len(selected_files)} ä¸ªæ–°æ–‡ä»¶ ID ä¿å­˜åˆ° {processed_file_path}")

    with open(keywords_file, "w", encoding="utf-8") as f:
        for keyword in available_keywords:
            f.write(keyword + "\n")
    print(f"âœ… å·²ç”¨å‰©ä½™çš„å…³é”®è¯æ›´æ–° {keywords_file}")

# ------------------------
# ç”Ÿæˆç´¯ç§¯çš„ç«™ç‚¹åœ°å›¾
# ------------------------
existing_html_files = [f for f in os.listdir(".") if f.endswith(".html") and f != "index.html"]
index_content = "<!DOCTYPE html><html><head><meta charset='utf-8'><title>Reading Glasses</title></head><body>\n"
index_content += "<h1>Reading Glasses</h1>\n<ul>\n"
for fname in sorted(existing_html_files):
    index_content += f'<li><a href="{fname}">{fname}</a></li>\n'
index_content += "</ul>\n</body></html>"

with open("index.html", "w", encoding="utf-8") as f:
    f.write(index_content)
print("âœ… å·²ç”Ÿæˆ index.html (å®Œæ•´ç«™ç‚¹åœ°å›¾)")

# ------------------------
# åœ¨æ¯ä¸ªé¡µé¢åº•éƒ¨æ·»åŠ éšæœºå†…éƒ¨é“¾æ¥ (å·²ä¼˜åŒ–ï¼Œä¸ä¼šç´¯ç§¯)
# ------------------------
all_html_files = [f for f in os.listdir(".") if f.endswith(".html") and f != "index.html"]

for fname in all_html_files:
    try:
        with open(fname, "r", encoding="utf-8", errors="replace") as f:
            content = f.read()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤æ‰€æœ‰å·²æœ‰çš„ footer é“¾æ¥éƒ¨åˆ†
        # re.DOTALL å…è®¸ '.' åŒ¹é…æ¢è¡Œç¬¦ï¼Œre.IGNORECASE å¿½ç•¥å¤§å°å†™
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä» <footer> åˆ° </footer> ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰
        content = re.sub(r"<footer>.*?</footer>", "", content, flags=re.DOTALL | re.IGNORECASE)
        
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„å¤šä½™çš„HTMLç»“æ„ï¼ˆå¤„ç†åµŒå¥—çš„HTMLé—®é¢˜ï¼‰
        content = re.sub(r"</body>\s*</html>\s*(?=<footer>|</body>)", "", content, flags=re.IGNORECASE)
        
        # ä»æ½œåœ¨é“¾æ¥åˆ—è¡¨ä¸­æ’é™¤å½“å‰æ–‡ä»¶
        other_files = [x for x in all_html_files if x != fname]
        # ç¡®å®šè¦æ·»åŠ çš„éšæœºé“¾æ¥æ•°é‡ï¼ˆ4 åˆ° 6 ä¸ªä¹‹é—´ï¼‰
        num_links = min(len(other_files), random.randint(4, 6))

        if num_links > 0:
            random_links = random.sample(other_files, num_links)
            links_html = "<footer><ul>\n" + "\n".join([f'<li><a href="{x}">{x}</a></li>' for x in random_links]) + "\n</ul></footer>"
            
            # ç¡®ä¿åªä¿ç•™æœ€åä¸€ä¸ª</body></html>æ ‡ç­¾
            content = re.sub(r"</body>\s*</html>.*$", "", content, flags=re.IGNORECASE)
            content = content.strip() + "\n" + links_html + "</body></html>"

        with open(fname, "w", encoding="utf-8") as f:
            f.write(content)
    except Exception as e:
        print(f"æ— æ³•ä¸º {fname} å¤„ç†å†…éƒ¨é“¾æ¥: {e}")

print("âœ… å·²ä¸ºæ‰€æœ‰é¡µé¢æ›´æ–°åº•éƒ¨éšæœºå†…éƒ¨é“¾æ¥ (æ¯ä¸ª 4-6 ä¸ªï¼Œå®Œå…¨åˆ·æ–°)")


# ------------------------
# éƒ¨ç½²éƒ¨åˆ† (æ–°)
# ------------------------
deploy_targets_file = "deploy_targets.json"

try:
    if os.path.exists(deploy_targets_file):
        with open(deploy_targets_file, "r") as f:
            deploy_targets = json.load(f)
            if not isinstance(deploy_targets, list) or not deploy_targets:
                raise ValueError("deploy_targets.json æ ¼å¼ä¸æ­£ç¡®ï¼Œå®ƒåº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å«ç›®æ ‡çš„éç©ºåˆ—è¡¨ã€‚")
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿›å…¥ API åˆ›å»ºæ¨¡å¼
        print(f"âŒ æœªæ‰¾åˆ° {deploy_targets_file} æ–‡ä»¶ã€‚")
        deploy_targets = [create_new_target_api(
            os.environ.get("VERCEL_TOKEN"),
            os.environ.get("NETLIFY_TOKEN"),
            os.environ.get("VERCEL_ORG_ID")
        )]

    # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„è½®å¾ªæ–¹æ³•æ¥é€‰æ‹©ç›®æ ‡
    current_target_index_file = "current_target_index.txt"
    current_index = 0
    if os.path.exists(current_target_index_file):
        try:
            with open(current_target_index_file, "r") as f:
                current_index = int(f.read().strip())
        except (IOError, ValueError):
            pass

    target_index_to_use = current_index % len(deploy_targets)
    selected_target = deploy_targets[target_index_to_use]

    print(f"ğŸ¯ æ­£åœ¨ä½¿ç”¨ç›®æ ‡ç´¢å¼• {target_index_to_use} è¿›è¡Œéƒ¨ç½²ã€‚")
    deploy_to_target(selected_target)

    # æ›´æ–°ç´¢å¼•ä»¥ä¾¿ä¸‹æ¬¡è¿è¡Œ
    with open(current_target_index_file, "w") as f:
        f.write(str(target_index_to_use + 1))
except (json.JSONDecodeError, ValueError) as e:
    print(f"âŒ è¯»å–æˆ–è§£æ {deploy_targets_file} æ—¶å‡ºé”™: {e}")
    sys.exit(1)
