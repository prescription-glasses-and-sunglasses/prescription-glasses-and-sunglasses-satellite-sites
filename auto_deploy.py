import os
import io
import json
import random
import time
from googleapiclient.http import MediaIoBaseDownload
from googleapiclient.discovery import build

# ------------------------
# é…ç½®
# ------------------------
FOLDER_IDS = []  # å¯ä¸ºç©ºï¼Œæ¯æ¬¡è‡ªåŠ¨æ‹‰å– Google Drive æ‰€æœ‰æ–‡ä»¶
processed_file_path = "processed_files.json"
keywords_file = "keywords.txt"

# ------------------------
# Google Drive API åˆå§‹åŒ–
# ------------------------
# å‡è®¾ä½ å·²ç»æœ‰ service å¯¹è±¡
# service = build('drive', 'v3', credentials=creds)

def get_cached_files():
    if os.path.exists(processed_file_path):
        with open(processed_file_path, "r", encoding="utf-8") as f:
            return json.load(f).get("all_files", None)
    return None

def save_files_to_cache(all_files):
    data = {"all_files": all_files, "fileIds": []}
    with open(processed_file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4)

def list_files(folder_id):
    results = service.files().list(
        q=f"'{folder_id}' in parents and trashed = false",
        pageSize=100,
        fields="files(id, name, mimeType)"
    ).execute()
    return results.get('files', [])

def download_html_file(file_id, file_name):
    request = service.files().get_media(fileId=file_id)
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    print(f"âœ… å·²ä¸‹è½½ {file_name}")

def download_txt_file(file_id, file_name, original_name):
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    text_content = fh.getvalue().decode('utf-8')
    
    is_html = text_content.strip().lower().startswith('<!doctype html') or text_content.strip().lower().startswith('<html')
    
    if is_html:
        html_content = text_content
    else:
        html_content = f"<!DOCTYPE html><html><head><meta charset='utf-8'><title>{original_name}</title></head><body><pre>{text_content}</pre></body></html>"
    
    with open(file_name, 'w', encoding='utf-8') as f:
        f.write(html_content)
    print(f"âœ… TXT å·²è½¬æ¢ä¸º HTML: {file_name}")

def export_google_doc(file_id, file_name):
    request = service.files().export_media(fileId=file_id, mimeType='text/html')
    fh = io.FileIO(file_name, 'wb')
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        _, done = downloader.next_chunk()
    print(f"âœ… Google æ–‡æ¡£å·²å¯¼å‡ºä¸º HTML: {file_name}")

# ------------------------
# ä¸»ç¨‹åº
# ------------------------
processed_data = {"fileIds": []}
if os.path.exists(processed_file_path):
    with open(processed_file_path, "r") as f:
        processed_data = json.load(f)

# åŠ è½½ keywords
if os.path.exists(keywords_file):
    with open(keywords_file, "r", encoding="utf-8") as f:
        keywords = [line.strip() for line in f if line.strip()]
else:
    keywords = []

# æ‹‰å–æ–‡ä»¶åˆ—è¡¨
all_files = get_cached_files()
if all_files is None:
    all_files = []
    print("â³ æ­£åœ¨ä» Google Drive æ‹‰å–æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨...")
    for folder_id in FOLDER_IDS:
        print(f"ğŸ“‚ æ­£åœ¨è·å–æ–‡ä»¶å¤¹: {folder_id}")
        files = list_files(folder_id)
        all_files.extend(files)
    save_files_to_cache(all_files)

new_files = [f for f in all_files if f['id'] not in processed_data.get("fileIds", [])]

if not new_files:
    print("âœ… æ²¡æœ‰æ–°çš„æ–‡ä»¶éœ€è¦å¤„ç†ã€‚")
else:
    print(f"å‘ç° {len(new_files)} ä¸ªæœªå¤„ç†æ–‡ä»¶ã€‚")
    num_to_process = min(len(new_files), 30)
    selected_files = random.sample(new_files, num_to_process)
    print(f"æœ¬æ¬¡è¿è¡Œå°†å¤„ç† {len(selected_files)} ä¸ªæ–‡ä»¶ã€‚")

    available_keywords = list(keywords)
    keywords_ran_out = False

    for f in selected_files:
        if available_keywords:
            keyword = available_keywords.pop(0)
            safe_name = keyword + ".html"
        else:
            if not keywords_ran_out:
                print("âš ï¸ å…³é”®è¯å·²ç”¨å®Œï¼Œå°†ä½¿ç”¨åŸå§‹æ–‡ä»¶ååŠ éšæœºåç¼€ã€‚")
                keywords_ran_out = True
            
            base_name = os.path.splitext(f['name'])[0]
            sanitized_name = base_name.replace(" ", "-").replace("/", "-")
            random_suffix = str(random.randint(1000, 9999))
            safe_name = f"{sanitized_name}-{random_suffix}.html"

        print(f"æ­£åœ¨å¤„ç† '{f['name']}' -> '{safe_name}'")

        if f['mimeType'] == 'text/html':
            download_html_file(f['id'], safe_name)
        elif f['mimeType'] == 'text/plain':
            download_txt_file(f['id'], safe_name, f['name'])
        else:  # Google Doc
            export_google_doc(f['id'], safe_name)

        processed_data["fileIds"].append(f['id'])

    with open(processed_file_path, "w") as f:
        json.dump(processed_data, f, indent=4)
    print(f"ğŸ’¾ å·²å°† {len(selected_files)} ä¸ªæ–°æ–‡ä»¶ ID ä¿å­˜åˆ° {processed_file_path}")

    with open(keywords_file, "w", encoding="utf-8") as f:
        for keyword in available_keywords:
            f.write(keyword + "\n")
    print(f"âœ… å·²ç”¨å‰©ä½™çš„å…³é”®è¯æ›´æ–° {keywords_file}")

# ------------------------
# ç”Ÿæˆç«™ç‚¹åœ°å›¾ index.html
# ------------------------
existing_html_files = [f for f in os.listdir(".") if f.endswith(".html") and f != "index.html"]
index_content = "<!DOCTYPE html><html><head><meta charset='utf-8'><title>Reading Glasses</title></head><body>\n"
index_content += "<h1>Reading Glasses</h1>\n<ul>\n"
for fname in sorted(existing_html_files):
    index_content += f'<li><a href="{fname}">{fname}</a></li>\n'
index_content += "</ul>\n</body></html>"

with open("index.html", "w", encoding="utf-8") as f:
    f.write(index_content)
print("âœ… å·²ç”Ÿæˆ index.html (å®Œæ•´ç«™ç‚¹åœ°å›¾)")

# ------------------------
# éšæœºéƒ¨ç½²åˆ° Vercel / Netlify
# ------------------------
VERCEL_TOKEN = os.environ.get("VERCEL_TOKEN")
NETLIFY_TOKEN = os.environ.get("NETLIFY_TOKEN")

# éšæœºé€‰æ‹© 10 æ¡éƒ¨ç½²åˆ° Vercel
vercel_count = min(10, len(existing_html_files))
vercel_files = random.sample(existing_html_files, vercel_count)

# å‰©ä½™æ–‡ä»¶éšæœºé€‰æ‹© 10 æ¡éƒ¨ç½²åˆ° Netlify
remaining_files = [f for f in existing_html_files if f not in vercel_files]
netlify_count = min(10, len(remaining_files))
netlify_files = random.sample(remaining_files, netlify_count)

def deploy_vercel(file_list):
    for f in file_list:
        headers = {"Authorization": f"Bearer {VERCEL_TOKEN}"}
        # requests.post("https://api.vercel.com/v1/.../deploy", headers=headers, files={"file": open(f, "rb")})
        print(f"éƒ¨ç½²åˆ° Vercel: {f}")
        time.sleep(0.5)

def deploy_netlify(file_list):
    for f in file_list:
        headers = {"Authorization": f"Bearer {NETLIFY_TOKEN}"}
        # requests.post("https://api.netlify.com/api/v1/sites/{site_id}/deploys", headers=headers, files={"file": open(f, "rb")})
        print(f"éƒ¨ç½²åˆ° Netlify: {f}")
        time.sleep(0.5)

deploy_vercel(vercel_files)
deploy_netlify(netlify_files)

print("âœ… æœ¬æ¬¡éšæœºéƒ¨ç½²å®Œæˆ")
